# Report

Firstly, I incorporated a string (company_name) processing pipeline to normalize variations in company names, such as abbreviations, which improved the overall accuracy and robustness of the matching process. The pipeline consists of steps: remove punctuation, map abbreviation to regular words, stem words, cut off stop words. For instance, using a keyword processor, abbreviations like "inc" were mapped to "incorporated," thus enhancing the matching quality and minimizing errors stemming from typographical variations. Or I used stemming to make company names containing words like “Computer” and “Computing” have better scores.

To achieve accurate and efficient fuzzy matching, I ultimately employed a K-Nearest Neighbors (KNN) approach, with TF-IDF vector embeddings and cosine similarity as the distance metric. TF-IDF captures term importance in context, effectively transforming each company name into a vector embedding that reflects character n-gram frequencies, enhancing match accuracy. Cosine similarity works well with these embeddings, identifying similarities and providing reliable rankings even for longer or complex names, such as accurately matching "Meta Platforms, Inc." with the related shorter name "Meta." Compared to methods, which I tried earlier, like the BK-tree with Levenshtein distance, which penalized longer names disproportionately, or the Ratcliff/Obershelp algorithm, which struggled with word reordering, the TF-IDF and KNN-based solution is faster and more scalable, especially suited for larger datasets due to its efficient retrieval capabilities. Limitations include potential inaccuracies in cases where character-based embeddings alone may not capture deeper semantic relationships; however, pre-processing steps like abbreviation normalization alleviate these challenges. Performance-wise, the model effectively returned correct matches, as seen with 'Apple Inc.', which matched closely with variations like "Apple Computer Inc." and "Computer Inc Apple."
